{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **演示1001：K近邻分类**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **提出问题**\n",
    "已知N维空间中若干个点的坐标，以及这些点所属的类别(子空间)。给定新的点坐标，如何判断该点应被划入哪个类别(子空间)？  \n",
    "![](../images/100101.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **分析问题**\n",
    "* KNN算法基本思想：\n",
    "已知一批数据集及其对应的分类标签，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类。具体步骤：  \n",
    "(1) 计算测试数据与各个训练数据之间的距离；  \n",
    "(2) 按照距离的递增关系进行排序；  \n",
    "(3) 选取距离最小的K个点；  \n",
    "(4) 确定前K个点所在类别的出现频率；  \n",
    "(5) 返回前K个点中出现频率最高的类别作为测试数据的预测分类。  \n",
    "与之前线性回归、逻辑回归和朴素贝叶斯模型不同，KNN算法不会形成假设函数。每次预测时，必须即时跟所有训练数据进行计算，因此工作量很大。\n",
    "\n",
    "* K值的选取：\n",
    " * K可以视为一个hyper-parameter(超参数)，一般需要通过交叉验证的方法来选取最优值\n",
    " * 如果K值太小就意味着整体模型变得复杂，容易发生过拟合(High Variance)，即如果邻近的实例点恰巧是噪声，预测就会出错，极端的情况是K=1，称为最近邻算法，对于待预测点x，与x最近的点决定了x的类别\n",
    " * K值的增大意味着整体的模型变得简单，极端的情况是K=N，那么无论输入实例是什么，都简单地预测它属于训练集中最多的类。这样的模型过于简单，容易发生欠拟合(High Bias)\n",
    "\n",
    "* K-D树方法：\n",
    " * K近邻法的最简单实现是线性扫描（又称暴力法），这时要计算输入实例与每一个训练实例的距离，当训练集很大时，计算非常耗时，这种方法是不可行的\n",
    " * 为了提高K近邻搜索的效率，可以考虑使用特殊的结构存储训练数据，以减少计算距离的次数。K-D树提供了一种最基本的方法\n",
    " * 可将K-D树看成类似于二叉查找树的数据结构，每个训练数据存储在这个数据结构中。在查找数据时，采用类似于中序遍历的算法。（但实际过程要复杂许多）。\n",
    " * K-D树搜索的平均复杂度为$O(logM)$。其中,M为样本数量。但是当Feature数量N很大时，搜索性能将急剧下降。一般K-D树适于M>>N的情形\n",
    "\n",
    "* KNN的优缺点：\n",
    " * 精度较高，对异常值不太敏感\n",
    " * 特别适合多分类的情况\n",
    " * 简单易实现\n",
    " * 很多情况下比朴素贝叶斯的效果更好\n",
    " * 计算复杂度高，空间复杂度高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **案例1：实现简单的KNN算法**\n",
    "> 给定若干组数据及其对应分类，再给定一组测试数据，使用KNN计算出该测试数据最接近的分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试数据为: [1.1 0.3] 分类结果为： B\n"
     ]
    }
   ],
   "source": [
    "''' 手工实现KNN算法进行简单的数据分类 '''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 给出训练数据以及对应的类别\n",
    "def createDataSet():\n",
    "    group = np.array([[1.0, 2.0], [1.2, 0.1], [0.1, 1.4], [0.3, 3.5]])\n",
    "    labels = ['A', 'B', 'C', 'D']\n",
    "    return group,labels\n",
    "\n",
    "# 通过KNN进行分类\n",
    "def classify(input, dataSet, label, k):\n",
    "    dataSize = dataSet.shape[0]\n",
    "    # 计算欧式距离\n",
    "    # tile将input按照(dataSize, 1)的shape进行复制，使之与dataSet的shape一致，从而可以对应元素相减\n",
    "    diff = np.tile(input, (dataSize, 1)) - dataSet\n",
    "    sqdiff = diff ** 2\n",
    "    squareDist = np.sum(sqdiff, axis=1)             # 行向量分别相加，从而得到新的一个行向量\n",
    "    dist = squareDist ** 0.5\n",
    "    \n",
    "    # argsort()根据元素的值从大到小对元素进行排序，返回下标\n",
    "    sortedDistIndex = np.argsort(dist)              \n",
    "\n",
    "    classCount = {}\n",
    "    for i in range(k):\n",
    "        voteLabel = label[sortedDistIndex[i]]\n",
    "        # 对选取的K个样本所属的类别个数进行统计\n",
    "        # classCount.get(voteLabel, 0)：如果voteLabel在classCount已经存在，则返回它对应值\n",
    "        # 如果不存在，则返回默认值0\n",
    "        classCount[voteLabel] = classCount.get(voteLabel, 0) + 1\n",
    "    \n",
    "    # 选取出现的类别次数最多的类别\n",
    "    maxCount = 0\n",
    "    for key,value in classCount.items():\n",
    "        if value > maxCount:\n",
    "            maxCount = value\n",
    "            classes = key\n",
    "\n",
    "    return classes   \n",
    "\n",
    "\n",
    "dataSet, labels = createDataSet()\n",
    "input = np.array([1.1, 0.3])\n",
    "K = 3\n",
    "output = classify(input, dataSet, labels, K)\n",
    "print(\"测试数据为:\",input,\"分类结果为：\",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **案例2：使用sklearn.neighbors.NearestNeighbors**\n",
    "* NearestNeightbors能够直接获得最接近的K个分类(以该分类对应的下标索引形式返回)，以及对应的距离\n",
    "* 选择KNN的实现算法(指定algorithm参数)\n",
    " * 'brute'：暴力法\n",
    " * 'kd-tree'\n",
    " * 'ball-tree'\n",
    "* 设置权重模式(指定weights参数)\n",
    " * 'uniform'：所有点的权重相同\n",
    " * 'distance'：越接近的点，权重越大\n",
    " * [callable]：允许通过一个自定义的函数来确定各个点的权重\n",
    "* 设置距离计算模式(指定metric参数)\n",
    " * 默认为'minkowski'。其它可选如：euclidean,manhattan,chebyshev等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试数据为: [[1.1 0.3]]\n",
      "测试数据返回的索引集合尺寸： (1, 3)\n",
      "分类结果，由近及远依次为： B C A\n"
     ]
    }
   ],
   "source": [
    "''' 使用NearestNeighbors进行分类 '''\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# 给出训练数据以及对应的类别\n",
    "def createDataSet():\n",
    "    group = np.array([[1.0, 2.0], [1.2, 0.1], [0.1, 1.4], [0.3, 3.5]])\n",
    "    labels = ['A', 'B', 'C', 'D']\n",
    "    return group,labels\n",
    "\n",
    "dataSet, labels = createDataSet()\n",
    "K = 3\n",
    "model = NearestNeighbors(n_neighbors=K, algorithm='ball_tree')\n",
    "model.fit(dataSet)\n",
    "input = np.array([[1.1, 0.3]])\n",
    "distances, indices = model.kneighbors(input)\n",
    "print(\"测试数据为:\",input)\n",
    "print(\"测试数据返回的索引集合尺寸：\", indices.shape)\n",
    "print(\"分类结果，由近及远依次为：\",labels[indices[0,0]],labels[indices[0,1]],labels[indices[0,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **案例3：使用sklearn.neighbors.NearestNeighbors识别手写数字图片**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "装载训练数据： 5000 条，训练中......\n",
      "训练完毕\n",
      "装载测试数据： 500 条，预测中......\n",
      "测试数据返回的索引矩阵尺寸： (500, 3)\n",
      "预测完毕。错误： 28 条\n",
      "测试数据正确率: 0.944\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import collections as col\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "trainData = np.loadtxt(open('digits_training.csv', 'r'), delimiter=\",\",skiprows=1)\n",
    "MTrain, NTrain = np.shape(trainData)\n",
    "xTrain = trainData[:, 1:NTrain]\n",
    "yTrain = trainData[:, 0]\n",
    "print(\"装载训练数据：\", MTrain, \"条，训练中......\")\n",
    "\n",
    "K = 3\n",
    "model = NearestNeighbors(n_neighbors=K, algorithm='auto')\n",
    "model.fit(xTrain)\n",
    "print(\"训练完毕\")\n",
    "\n",
    "testData = np.loadtxt(open('digits_testing.csv', 'r'), delimiter=\",\",skiprows=1)\n",
    "MTest,NTest = np.shape(testData)\n",
    "xTest = testData[:, 1:NTest]\n",
    "yTest = testData[:, 0]\n",
    "print(\"装载测试数据：\", MTest, \"条，预测中......\")\n",
    "\n",
    "indices = model.kneighbors(xTest, return_distance=False)\n",
    "print(\"测试数据返回的索引矩阵尺寸：\", indices.shape)\n",
    "yPredicts = np.zeros(MTest)\n",
    "for i in np.arange(0, MTest):\n",
    "    counter = col.Counter(indices[i])               # 统计最近的K个索引分别出现的次数\n",
    "    max_index = counter.most_common()[0][0]         # 获取次数最多的那个索引值\n",
    "    yPredicts[i] = yTrain[max_index]\n",
    "\n",
    "errors = np.count_nonzero(yTest - yPredicts)\n",
    "print(\"预测完毕。错误：\", errors, \"条\")\n",
    "print(\"测试数据正确率:\", (MTest - errors) / MTest)   # 约0.944的正确率"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
