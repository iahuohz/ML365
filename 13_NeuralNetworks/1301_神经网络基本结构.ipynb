{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **演示1301：神经网络基本结构**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **前馈神经网络**\n",
    "前馈神经网络是一种最简单的神经网络，各神经元分层排列。每个神经元只与前一层的神经元相连。接收前一层的输出，并输出给下一层．各层间没有反馈。适于处理复杂的非线性分类情况。相比线性回归、logistic回归，提高灵活性的同时，又不太会有过拟合。  \n",
    "对于一组训练数据$(x,y)$，其中，$x$包含4个Feature，分别记为$x_1,x_2,x_3,x_4$；如下图所示的神经网络一共分为4层：  \n",
    "![](../images/130101.png)  \n",
    "\n",
    "* 各层解析\n",
    " * $x$也被称为神经网络的Layer 1(第一层)，又称：Input Layer。该层也可以记为$a^{(1)}$ 。其中$x_1,x_2,x_3,x_4$ 分别也可记为：$a_1^{(1)},a_2^{(1)},a_3^{(1)},a_4^{(1)}$\n",
    " * $a^{(2)}$ 是网络中的Layer 2, $a^{(3)}$ 是网络中的Layer 3。这两层又称：Hidden Layer\n",
    " * $a^{(4)}$是网络中的Layer 4，也就是最后一层。该层又称：Output Layer，它就是Hypothesis函数\n",
    " * 本例中Output Layer最终只有1个元素。此时该元素代表该组$x$所对应的$y$值是1的几率。在Multi Classfication中，Output Layer可能有多个元素，那么第$k$个元素就代表该组$x$所对应的$y$值是$k$的几率。取几率最大的那个$k$值作为预测结果。\n",
    " * $a_0$或$x_0$称为Bias Unit，一般赋值为1。（类似于Linear Regression和Logistic Regression中的Intercept Item）\n",
    "* 计算任务\n",
    " * 从$a^{(1)}$到$a^{(2)}$，需要乘以第一层模型参数$\\theta^{(1)}$；从$a^{(2)}$到$a^{(3)}$，需要乘以第二层模型参数$\\theta^{(2)}$；从$a^{(3)}$ 到$a^{(4)}$，需要乘以第二层模型参数$\\theta^{(3)}$\n",
    " * 请注意，后一层节点中的每个元素，都跟前一层节点的所有元素有关。每一层节点均可视为具有$N$个元素的列向量\n",
    " * 每层的$\\theta^{(i)}$都是一个$N^{(i+1)} \\times N^{(i)}$ 矩阵，其中$N^{(i)}$ 表示前一层的节点数量(含Interception Item)；$N^{(i+1)}$ 表示后一层的节点数量(不包含Interception Item)。以下将该矩阵写成符号： $ \\Theta $\n",
    "* 完整计算过程\n",
    " * 先给定$\\Theta$的初始值，然后从输入层开始，执行正向计算(Foword Propagation)，直到计算出初始的$h_\\theta(x)$\n",
    " * 根据Cost Function计算此时的Cost\n",
    " * 使用反向计算(Backpropagation Algorithm)来计算偏导数（梯度），执行梯度递减算法\n",
    " * 循环执行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **前向计算各层参数计算**\n",
    "1. 根据$x$和$\\Theta^{(1)}$ 计算$a^{(2)}$ 的值：  \n",
    "$ a_1^{(2)}=g(\\Theta_{10}^{(1)} x_0+ \\Theta_{11}^{(1)} x_1+ \\Theta_{12}^{(1) } x_2+\\Theta_{13}^{(1)} x_3+ \\Theta_{14}^{(1)} x_4 ) $  \n",
    "$ a_2^{(2)}=g(\\Theta_{20}^{(1)} x_0+ \\Theta_{21}^{(1)} x_1+ \\Theta_{22}^{(1)} x_2+\\Theta_{23}^{(1)} x_3+ \\Theta_{24}^{(1)} x_4) $  \n",
    "$ a_3^{(2)}=g(\\Theta_{30}^{(1)} x_0+ \\Theta_{31}^{(1)} x_1+ \\Theta_{32}^{(1)} x_2+\\Theta_{33}^{(1)} x_3+ \\Theta_{34}^{(1)} x_4 ) $  \n",
    "$g$仍然代表sigmoid函数：$g(z)=\\dfrac{1}{1+e^{-z}}$  \n",
    "采用矩阵运算可表达为：$a^{(2)}=g(\\Theta^{(1)} * a^{(1)})$  \n",
    "   * $a_i^{(j)}$表示第$j$层的第$i$个元素值。其中$a_i^{(1)}$ 代表输入的Features(X)中的第$i$个元素。$a^{(1)}$或$X$是一个具有$N$个元素的列向量\n",
    "   * $\\Theta$表示权重矩阵(Matrix of Weights)。$\\Theta^{(j)}$ 表示从第$j$层到第$j+1$层的权重矩阵。\n",
    "   * $\\Theta_{ik}^{(j)}$ 表示从第$j$层到第$j+1$层的权重矩阵中的第$i$行第$k$列权重值。$i$实际上对应着$j+1$层中的第$i$个元素；而$k$对应着第$j$层的第$k$个元素。\n",
    "   * 第一个计算式$a_1^{(2)}$，可参见见上图中的红色箭头线，这5条线依次代表从第1层的第0、1、2、3、4号元素分别映射到第2层的第1号元素。第一条红线代表的权重矩阵元素就是：$\\Theta_{10}^{(1)}$，第二条红线代表的是：$\\Theta_{11}^{(1)}$，依次类推。  \n",
    "2. 根据$a^{(2)}$和$\\Theta^{(2)}$计算$a^{(3)}$ 的值：  \n",
    "$ a_1^{(3)}=g(\\Theta_{10}^{(2)} a_0^{(2)}+ \\Theta_{11}^{(2)} a_1^{(2)}+ \\Theta_{12}^{(2)} a_2^{(2)}+\\Theta_{13}^{(2)} a_3^{(2)}) $  \n",
    "$ a_2^{(3)}=g(\\Theta_{20}^{(2)} a_0^{(2)}+ \\Theta_{21}^{(2)} a_1^{(2)}+ \\Theta_{22}^{(2)} a_2^{(2)}+\\Theta_{23}^{(2)} a_3^{(2)} ) $  \n",
    "其中，$a_0$为Bias Unit，一般赋值为1。  \n",
    "采用矩阵运算可表达为：$a^{(3)}=g(\\Theta^{(2)} * a^{(2)}) $  \n",
    "3. 根据$a^{(3)}$ 和$\\Theta^{(3)}$ 计算$a^{(4)}$ 的值：  \n",
    "$ a_1^{(4)}=g(\\Theta_{10}^{(3)} a_0^{(3)}+ \\Theta_{11}^{(3)} a_1^{(3)}+ \\Theta_{12}^{(3)} a_2^{(3)}) $  \n",
    "采用矩阵运算可表达为：$a^{(4)}=g(\\Theta^{(3)} * a^{(3)}) $  \n",
    "最终可以计算出预测的Hypothesis值：$h_\\theta(x)= a_1^{(4)}$  \n",
    "上述结果就是针对$x$预测其属于其对应的$y$值的几率。  \n",
    "4. 通过向量/矩阵运算的维度来验证计算是否正确：\n",
    "   * 如果第$j$层有$M$个有效元素(不含Unit Bias)，第$j+1$层有$K$个元素。则从第$j$层到第$j+1$层的矩阵$\\Theta^{(j)}$ 大小为：$K \\times (M+1)$\n",
    "   * 上述过程可以看到，只要计算出$\\Theta$，就能根据输入的$x$预测出其对应的$y$值。\n",
    "   * 请注意，上述计算仅仅还只是针对一组训练数据。事实上，针对每一组训练数据，都需要执行上述过程以便累积出该轮的Cost，然后再采用Back Propagation方法进行梯度递减计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **多个分类输出的神经网络**\n",
    "![](../images/130102.png)  \n",
    "本模型中，$a^{(4)}$ 包括$K$个可能的类别输出。  \n",
    "例如，如果$K=4$，则$y$的值可能为下列4个向量之一：  \n",
    "$ \\left(\\begin{matrix} 1\\\\0\\\\0\\\\0 \\end{matrix}\\right) $$，$$ \\left(\\begin{matrix} 0\\\\1\\\\0\\\\0 \\end{matrix}\\right) $$，$$ \\left(\\begin{matrix} 0\\\\0\\\\1\\\\0 \\end{matrix}\\right) $$，$$ \\left(\\begin{matrix} 0\\\\0\\\\0\\\\1 \\end{matrix}\\right) $  \n",
    "$ \\begin{aligned}\n",
    "J(\\theta)&=\\dfrac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^K [(-y_k^{(i)} log(h_\\theta(x^{(i)})_k )-(1-y_k^{(i)}) log(1 - h_\\theta(x^{(i)})_k)] \\\\\n",
    "& + \\dfrac{\\lambda}{2m} \\sum_{l=1}^{L-1} \\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}(\\Theta_{ji}^{(l)})^2\n",
    "\\end{aligned} $\n",
    " * $K$：$h_\\theta(x)$的可能结果数目（分类数)，也就是Output Layer的节点个数\n",
    " * $L$：神经网络的层数（包括Input layer, Hidden layer, Output layer)\n",
    " * $s_l$：第$l$层节点个数 (不包括bias unit)\n",
    " * $y_k$：代表$y$实际结果向量中第$k$个元素的值(0或1)\n",
    " * $h_\\theta(x^{(i)})_k$：判别式输出结果向量中的第$k$个元素的值(0~1之间)\n",
    " * 在Regularization项中，不应包含Bias Unit项参数（也就是说，$\\Theta_{j0}^{(l)}$ 不出现在Penalty项中）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **案例1：使用sklearn.neural_network.MLPClassifier类实现手写数字图片识别**\n",
    "* 该类实现了前馈计算并且通过Back Propagation算法计算梯度\n",
    "* 几个重要属性\n",
    " * alpha：代表Penalty项中的$\\lambda$\n",
    " * hidden_layer_sizes：分别指定hidden layer中每一层的节点数(不包括Bias Unit)\n",
    " * activation：指定激活函数的类型\n",
    "* 很多情况下，神经网络的计算需要花费较多时间，因此在训练完成后，需要保存模型数据\n",
    " * joblib.dump方法用于保存模型参数\n",
    " * joblib.load方法用于从文件种装载模型参数构造一个MLPClassifier对象\n",
    "* score方法用于估算正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "装载训练数据： 5000 条，训练中......\n",
      "训练完毕，保存模型...\n",
      "模型保存完毕，执行测试...\n",
      "装载测试数据： 500 条，预测中......\n",
      "预测完毕。错误： 36 条\n",
      "测试数据正确率: 0.928\n",
      "模型内建的正确率估计： 0.928\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def normalizeData(X):\n",
    "    return (X - X.mean())/X.max()\n",
    "\n",
    "trainData = np.loadtxt(open('digits_training.csv', 'r'), delimiter=\",\",skiprows=1)\n",
    "MTrain, NTrain = np.shape(trainData)\n",
    "xTrain = trainData[:,1:NTrain]\n",
    "xTrain = normalizeData(xTrain)         \n",
    "yTrain = trainData[:,0]\n",
    "print(\"装载训练数据：\", MTrain, \"条，训练中......\")\n",
    "\n",
    "model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(48, 24), random_state=1)\n",
    "model.fit(xTrain, yTrain)   \n",
    "\n",
    "print(\"训练完毕，保存模型...\")\n",
    "joblib.dump(model, \"mlp_classifier_model1.m\")                 # 保存模型到文件中\n",
    "print(\"模型保存完毕，执行测试...\")\n",
    "\n",
    "testData = np.loadtxt(open('digits_testing.csv', 'r'), delimiter=\",\",skiprows=1)\n",
    "MTest,NTest = np.shape(testData)\n",
    "xTest = testData[:,1:NTest]\n",
    "xTest = normalizeData(xTest)\n",
    "yTest = testData[:,0]\n",
    "print(\"装载测试数据：\", MTest, \"条，预测中......\")\n",
    "\n",
    "model = joblib.load(\"mlp_classifier_model1.m\")                # 从之前保存的模型中装载参数\n",
    "yPredict = model.predict(xTest)\n",
    "errors = np.count_nonzero(yTest - yPredict)\n",
    "print(\"预测完毕。错误：\", errors, \"条\")\n",
    "print(\"测试数据正确率:\", (MTest - errors) / MTest)\n",
    "model_accuracy = model.score(xTest, yTest)\n",
    "print(\"模型内建的正确率估计：\", model_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
