{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **演示1302：反向传播算法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **单组训练数据，无Regulization项的反向传播梯度推导**\n",
    "* 前向计算流  \n",
    "考虑一组训练数据$(x,y)$，并且暂不考虑Regulization项，则成本函数如下：  \n",
    "$ \\begin{aligned}\n",
    "J(\\theta)&=\\sum_{k=1}^K [-y_k log(h_\\theta(x)_k)-(1-y_k) log(1-h_\\theta(x)_k)]\\\\ \\\\&=\\sum_{k=1}^K [-y_k log(a_k^{(4)})-(1-y_k)log(1-a_k^{(4)})]\n",
    "\\end{aligned}$  \n",
    "假定$x$维度为：$(D, 1)$，$y$维度为：$(K, 1)$，也就是说，把它们视为列向量。则：正向计算时，各节点、权重的维度如下表格：  \n",
    "![](../images/130201.png)  \n",
    "上表中，假设权重矩阵中与Bias项对应的参数放在第一列  \n",
    "前向计算流程图如下图所示：  \n",
    "![](../images/130202.png)\n",
    "* 反向传播\n",
    " * 计算$\\dfrac{\\partial L}{\\partial a^{(4)}}$，维度：$(K, 1)$  \n",
    "$\\begin{aligned} \\dfrac{\\partial L}{\\partial a^{(4)}} & = \\sum_{k=1}^K \\dfrac{-y_k log(a_k^{(4)})-(1-y_k)log(1-a_k^{(4)})}{\\partial a_i^{(4)}} \\\\ \\\\&=\n",
    "\\dfrac{\\partial [-y_i log(a_i^{(4)})-(1-y_i)log(1-a_i^{(4)})]}{\\partial a_i^{(4)}} \\\\ \\\\ &=\n",
    "-y_i * \\dfrac{1}{a_i^{(4)}} -(1-y_i)*\\dfrac{1}{1-a_i^{(4)}} * (-1) \\\\ \\\\ &=\n",
    "\\dfrac{-y_i * (1-a_i^{(4)})+(1-y_i)*a_i^{(4)}}{a_i^{(4)}*(1-a_i^{(4)})} \\\\ \\\\& = \\dfrac{a_i^{(4)}-y_i}{a_i^{(4)}*(1-a_i^{(4)})}\n",
    "\\end{aligned} $  \n",
    "上式中，用到了下列事实：  \n",
    "   * 只有当$i=k$时，才会对$\\dfrac{\\partial L}{\\partial a_i^{(4)}}$有贡献，所以上述求和公式就可以直接化简\n",
    "   * 对数函数的导数：$f(x)=log(x)$，则：$f'(x)=1/x$  \n",
    "向量形式：  \n",
    "$ \\dfrac{\\partial L}{a^{(4)}}=\\dfrac{a^{(4)}-y}{a^{(4)} * (1-a^{(4)})} $  \n",
    "上式中，分母部分，$a^{(4)}*(1-a^{(4)})$是两个向量的对应元素分别相乘，并且仍然保留向量形式，然后再与分子部分进行对应元素相除。  \n",
    " * 计算$\\dfrac{\\partial L}{\\partial z^{(4)}}$ 维度：$(K, 1)$  \n",
    "考虑前向计算公式：$a_i^{(4)}=g(z_i^{(4)})$  \n",
    "$ \\dfrac{\\partial L}{\\partial z_i^{(4)}}=\\dfrac{\\partial L}{\\partial a_i^{(4)}} * \\dfrac{\\partial a_i^{(4)}}{\\partial z_i^{(4)}}=\\dfrac{\\partial L}{\\partial a_i^{(4)}} * \\dfrac{g(z_i^{(4)})}{\\partial z_i^{(4)}}=\\dfrac{a_i^{(4)}-y_i}{a_i^{(4)} * (1-a_i^{(4)})} * g(z_i^{(4)} ) * (1-g(z_i^{(4)}))=a_i^{(4)}-y_i $  \n",
    "上式中，用到了下列事实：  \n",
    "Sigmoid函数的导数：$g(x)=\\dfrac{1}{1+e^{-z}}$，则：$ g'(x)=g(x) * (1-g(x))$  \n",
    "向量形式：  \n",
    "$ \\delta^{(4)}=\\dfrac{\\partial L}{\\partial z^{(4)}}=a^{(4)}-y $\n",
    " * 计算$\\dfrac{\\partial L}{\\partial \\Theta^{(3)}}$，$\\dfrac{\\partial L}{\\partial a^{(3)}}$和$\\dfrac{\\partial L}{\\partial z^{(3)}}$ 维度分别为：$(K, H2+1)$，$(H2+1, 1)$，$(H2+1, 1)$  \n",
    "考虑前向计算公式：$z^{(4)}=\\Theta^{(3)} a^{(3)}$  \n",
    "考虑矩阵链式求导法则：若$Y=WX$，则$\\dfrac{\\partial L}{\\partial W}=\\dfrac{\\partial L}{\\partial Y} X^T$，$\\dfrac{\\partial L}{\\partial X}=W^T \\dfrac{\\partial L}{\\partial Y}$  \n",
    "可知：  \n",
    "$ \\dfrac{\\partial L}{\\partial \\Theta^{(3)}}=\\dfrac{\\partial L}{\\partial z^{(4)}} (a^{(3)})^T=\\delta^{(4)} (a^{(3)})^T $  \n",
    "$ \\dfrac{\\partial L}{\\partial a^{(3)}}=(\\Theta^{(3)})^T \\dfrac{\\partial L}{\\partial z^{(4)}}=(\\Theta^{(3)})^T \\delta^{(4)} $  \n",
    "$ \\delta_{(3)}=\\dfrac{\\partial L}{\\partial z^{(3)}}=\\dfrac{\\partial L}{\\partial a^{(3)}} * \\dfrac{\\partial a^{(3)}}{\\partial z^{(3)}}=(\\Theta^{(3)})^T \\delta^{(4)} * g(z^{(3)}) * (1-g(z^{(3)})) $  \n",
    "在计算$\\delta^{(3)}$ 时，一般要去掉$\\Theta^{(3)}$ 中与Bias项对应的权重项，例如，$\\Theta_{i0}^{(3)}$\n",
    " * 计算$\\dfrac{\\partial L}{\\partial \\Theta^{(2)}}$，$\\dfrac{\\partial L}{\\partial a^{(2)}}$ 和$\\dfrac{\\partial L}{\\partial z^{(2)}}$ 维度分别为：$(H2,H1)$，$(H1, 1)$，$(H1, 1)$  \n",
    "回顾前向计算公式：$z^{(3)}=\\Theta^{(2)} a^{(2)}$  \n",
    "可知：  \n",
    "$ \\dfrac{\\partial L}{\\partial \\Theta^{(2)}}=\\dfrac{\\partial L}{\\partial z^{(3)}} (a^{(2)})^T=\\delta^{(3)} (a^{(2)})^T $  \n",
    "$ \\dfrac{\\partial L}{\\partial a^{(2)}}=(\\Theta^{(2)})^T \\dfrac{\\partial L}{\\partial z^{(3)}}=(\\Theta^{(2)})^T \\delta^{(3)} $  \n",
    "$ \\delta_{(2)}=\\dfrac{\\partial L}{\\partial z^{(2)}}=\\dfrac{\\partial L}{\\partial a^{(2)}} * \\dfrac{\\partial a^{(2)}}{\\partial z^{(2)}}=(\\Theta{(2)})^T \\delta^{(3)} * g(z^{(2)}) * (1-g(z^{(2)})) $  \n",
    "在计算$\\delta^{(2)}$ 时，一般要去掉$\\Theta^{(2)}$ 中与Bias项对应的权重项，例如，$\\Theta_{i0}^{(2)}$\n",
    " * 计算$\\dfrac{\\partial L}{\\partial \\Theta^{(1)}}$  \n",
    "回顾正向计算公式：$z^{(1)}=\\Theta^{(1)} a^{(1)}$  \n",
    "$ \\dfrac{\\partial L}{\\partial \\Theta^{(1)}}=\\dfrac{\\partial L}{\\partial z^{(2)}} (a^{(1)})^T=\\delta^{(2)} (a^{(1)})^T $  \n",
    " * 反向传播导数流图：  \n",
    "![](../images/130203.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **多组样本反向传播计算**\n",
    "在$N$组样本数据的情况下，所有的梯度应该进行累积求和，然后取平均值  \n",
    "使用Python实现时，各节点、权重的维度如下表格：  \n",
    "![](../images/130204.png)  \n",
    "* 采用矩阵形式的前向计算  \n",
    "$ z^{(2)}=a^{(1)} (\\Theta^{(1)})^T $  \n",
    "$ a^{(2)}=[1,g(z^{(2)})] $ 注意：在$g(z^{(2)})$矩阵中，插入Bias项列  \n",
    "$ z^{(3)}=a^{(2)} (\\Theta^{(2)})^T $  \n",
    "$ a^{(3)}=[1,g(z^{(3)})] $ 注意：在$g(z^{(2)})$矩阵中，插入Bias项列  \n",
    "$ z^{(4)}=a^{(3)} (\\Theta^{(3)})^T $  \n",
    "$ a^{(4)}=g(z^{(3)}) $  \n",
    "* 采用矩阵形式的反向传播梯度计算\n",
    "$ \\delta^{(4)}=a^{(4)}-y $  \n",
    "$ \\delta^{(3)}=\\delta^{(4)} \\Theta^{(3)} * g(z^{(3)}) * (1-g(z^{(3)}) $ 注意：$\\Theta^{(3)}$中，Bias列$\\Theta_{i0}^{(3)}$不参与$\\delta^{(3)}$ 的计算。另外, $*$表示对应元素分别相乘(而不是矩阵运算)   \n",
    "$ \\delta^{(2)}=\\delta^{(3)} \\Theta^{(2)} * g(z^{(2)}) * (1-g(z^{(2)} )) $ 注意：$\\Theta^{(2)}$中，Bias列$\\Theta_{i0}^{(2)}$不参与$\\delta^{(2)}$的计算。另外,$*$表示对应元素分别相乘(而不是矩阵运算)    \n",
    "$ \\dfrac{\\partial L}{\\partial \\Theta^{(3)}}=\\dfrac{(\\delta^{(4)})^T a^{(3)}}{N} $  \n",
    "$ \\dfrac{\\partial L}{\\partial \\Theta^{(2)}}=\\dfrac{(\\delta^{(3)})^T a^{(2)}}{N} $  \n",
    "$ \\dfrac{\\partial L}{\\partial \\Theta^{(1)}}=\\dfrac{(\\delta^{(2)})^T a^{(1)}}{N} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **考虑Regulization项**\n",
    "$ \\dfrac{\\partial L}{\\partial \\Theta^{(3)}}=\\dfrac{(\\delta^{(4)})^T a^{(3) }}{N}+\\dfrac{\\lambda * \\Theta^{(3)}}{N} $  \n",
    "$ \\dfrac{\\partial L}{\\partial \\Theta^{(2)}}=\\dfrac{(\\delta^{(3)})^T a^{(2) }}{N}+\\dfrac{\\lambda * \\Theta^{(2)}}{N} $  \n",
    "$ \\dfrac{\\partial L}{\\partial \\Theta^{(1)}}=\\dfrac{(\\delta^{(2)})^T a^{(1) }}{N}+\\dfrac{\\lambda * \\Theta^{(1)}}{N} $  \n",
    "注意，上述所有计算的Regulization项，都不计入与Bias对应的权重项，例如$\\Theta_{i0}^{(3)}$,$\\Theta_{i0}^{(2)}$ 和$\\Theta_{i0}^{(1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **$\\Theta$的随机初始化：**\n",
    "* 不能将$\\Theta$初始化为0或1，或者相同的数。否则将会导致计算出来的$a^{(l)}$ 中的每个节点都相同，从而使原本的$n$个Feature变成相当于只有1个Feature。\n",
    "* 采用Symetric Breaking（random initialization) 方法可以解决上述问题。例如：  \n",
    "$ \\varepsilon_i\\_init=0.12 $  \n",
    "$ \\Theta^{(3)}=rand(s_2, s_1+1) * 2 * \\varepsilon_i\\_init - \\varepsilon_i\\_init $  \n",
    "$ \\Theta^{(2)}=rand(s_3, s_2+1) * 2 * \\varepsilon_i\\_init - \\varepsilon_i\\_init $  \n",
    "$ \\Theta^{(1)}=rand(s_{l+1}, s_l + 1) * 2 * \\varepsilon_i\\_init - \\varepsilon_i\\_init $  \n",
    "其中，rand函数用于生成0~1中的随机数。$s_i$是网络第$i$层的节点个数(不包括Bias Unit)；$s_i + 1$正好包含Bias Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gradient Checking**\n",
    "* Back Propagation算法非常容易出错(程序设计上的错误)，有时甚至每次迭代$J(\\Theta)$在不断减小，但仍然错误。因此需要有某种方式检查该算法编写是否正确\n",
    "* Gradient Checking是通过数值方式模拟计算$J(\\Theta)$在某个$\\Theta$处的导数：  \n",
    "$ f(\\Theta_{ij}^{(l)})=\\dfrac{J(\\Theta_{ij}^{(l)}+\\varepsilon)-J(\\Theta_{ij}^{(l)} - \\varepsilon)}{2 \\varepsilon}$  \n",
    "一般设置$\\varepsilon=0.0001$ 或更小  \n",
    "可以随机初始化一组$\\Theta_{ij}^{(l)}$数据，应包括每层、每行、每列的$\\Theta$参数（一组即可）  \n",
    "* 对比通过Back Propagation计算出来的某个导数$\\dfrac{\\partial}{\\Theta_{ij}^{(l)}} J(\\Theta)$与上述Gradient Checking计算出来的$f(\\Theta_{ij}^{(l)})$进行对比，它们之间的差值应至少小于$\\varepsilon$。这样才能说明Back Propagation的代码正确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **案例1：使用Back Propagation算法自行实现神经网络**\n",
    "* 使用了2个隐藏层，共需要3个$\\Theta$矩阵参数\n",
    "* 定义了cost和gradient方法，提供给scipy.optimize.minimize来优化计算Θ\n",
    "* 为了便于传递参数，将3个$\\Theta$矩阵展平成1个一维数组传递，然后在cost和gradient方法中还原成3个$\\Theta$矩阵\n",
    "* 本案例限制了minimize方法的最大循环次数为1000次，计算的cost值较小，但是数值上并未收敛\n",
    "* 调整$\\lambda$(lmd)的值，观察其收敛性。本例中，$\\lambda$越小，收敛越快，但是容易出现overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "装载训练数据： 5000 条\n",
      "训练完成：       fun: 0.02594138081052576\n",
      " hess_inv: <52650x52650 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([ 2.87006385e-06, -4.83466790e-10, -6.44375796e-10, ...,\n",
      "       -8.35536064e-06, -9.31126285e-07, -1.91863496e-06])\n",
      "  message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
      "     nfev: 566\n",
      "      nit: 539\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 5.07003161e-01, -2.41733395e-05, -3.22187898e-05, ...,\n",
      "        1.55242550e+00, -2.56950705e-01,  3.05867889e+00])\n",
      "装载测试数据： 500 条\n",
      "Accuracy: 0.946\n"
     ]
    }
   ],
   "source": [
    "''' 自定义反向传播算法实现 '''\n",
    "\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "\n",
    "def normalize_data(X):\n",
    "    return X/256\n",
    "\n",
    "def make_vectorized_labels(Y, classifies_count):\n",
    "    rows_count = len(Y)\n",
    "    vec_y = np.zeros((rows_count, classifies_count))\n",
    "    for i, v in enumerate(Y):\n",
    "        vec_y[i, v] = 1\n",
    "    return vec_y\n",
    "\n",
    "def sigmoid(X):\n",
    "    return 1.0 / (1.0 + np.exp(-X))\n",
    "\n",
    "def reshape_theta(THETA_1, THETA_2, THETA_3):\n",
    "    THETA_ALL = THETA_1.reshape(1, -1)\n",
    "    THETA_ALL = np.append(THETA_ALL, THETA_2.reshape(1, -1))\n",
    "    THETA_ALL = np.append(THETA_ALL, THETA_3.reshape(1, -1))\n",
    "    return THETA_ALL\n",
    "\n",
    "def extract_theta(THETA_ALL):\n",
    "    THETA_1 = THETA_ALL[0:h1_count*(input_count + 1)].reshape(h1_count, input_count + 1)\n",
    "    THETA_2 = THETA_ALL[h1_count*(input_count + 1):(h1_count*(input_count + 1) + h2_count*(h1_count + 1))].reshape(h2_count, h1_count + 1)\n",
    "    THETA_3 = THETA_ALL[-output_count*(h2_count + 1):].reshape(output_count, h2_count + 1)\n",
    "    return (THETA_1, THETA_2, THETA_3)\n",
    "\n",
    "def forward(THETA_1, THETA_2, THETA_3, X):\n",
    "    A_1 = X                                                                     # 5000x785\n",
    "    z_2 = np.dot(A_1, THETA_1.T)                                                # 5000x64\n",
    "    A_2 = np.c_[np.ones(A_1.shape[0]), sigmoid(z_2)]                            # 5000x65\n",
    "    z_3 = np.dot(A_2, THETA_2.T)                                                # 5000x32\n",
    "    A_3 = np.c_[np.ones(A_2.shape[0]), sigmoid(z_3)]                            # 5000x33\n",
    "    z_4 = np.dot(A_3, THETA_3.T)                                                # 5000x10\n",
    "    A_4 = sigmoid(z_4)                                                          # 5000x10\n",
    "    return A_4\n",
    "\n",
    "def cost(THETA_ALL, X, Y, lmd = 0):\n",
    "    THETA_1, THETA_2, THETA_3 = extract_theta(THETA_ALL)\n",
    "    A_4 = forward(THETA_1, THETA_2, THETA_3, X)\n",
    "    baseline = 1e-5                                 # 防止A_4中的元素值过于接近0或1，从而导致log运算溢出\n",
    "    A_4[np.where(A_4 < baseline)] = baseline\n",
    "    A_4[np.where(A_4 > 1 - baseline)] = 1 - baseline\n",
    "    cost1 = -np.sum(Y * np.log(A_4)) - np.sum((1 - Y) * np.log(1 - A_4))         \n",
    "    cost2 = np.sum(np.square(THETA_1[:, 1:])) + np.sum(np.square(THETA_2[:, 1:])) + np.sum(np.square(THETA_3[:, 1:]))\n",
    "    cost_all = (cost1 + lmd * cost2 / 2) / len(X)\n",
    "    return cost_all\n",
    "\n",
    "def grad(THETA_ALL, X, Y, lmd = 0):\n",
    "    THETA_1, THETA_2, THETA_3 = extract_theta(THETA_ALL)\n",
    "    A_1 = X                                                                     # 5000x785\n",
    "    z_2 = np.dot(A_1, THETA_1.T)                                                # 5000x64\n",
    "    sigmoid_z_2 = sigmoid(z_2)\n",
    "    A_2 = np.c_[np.ones(A_1.shape[0]), sigmoid_z_2]                             # 5000x65\n",
    "    z_3 = np.dot(A_2, THETA_2.T)                                                # 5000x32\n",
    "    sigmoid_z_3 = sigmoid(z_3)\n",
    "    A_3 = np.c_[np.ones(A_2.shape[0]), sigmoid_z_3]                             # 5000x33\n",
    "    z_4 = np.dot(A_3, THETA_3.T)                                                # 5000x10\n",
    "    A_4 = sigmoid(z_4)                                                          # 5000x10\n",
    "    d4 = A_4 - Y                                                                # 5000x10\n",
    "    d3 = np.dot(d4, THETA_3[:, 1:]) * sigmoid_z_3 * (1-sigmoid_z_3)             # 5000x32\n",
    "    d2 = np.dot(d3, THETA_2[:, 1:]) * sigmoid_z_2 * (1-sigmoid_z_2)             # 5000x64\n",
    "    M = len(X)\n",
    "    # DELTA_3 = np.zeros((output_count, h2_count + 1))\n",
    "    # DELTA_2 = np.zeros((h2_count, h1_count + 1))\n",
    "    # DELTA_1 = np.zeros((h1_count, input_count + 1))\n",
    "    # for i in range(M):\n",
    "    #     DELTA_3 += d4[i].reshape(-1,1) * A_3[i]\n",
    "    #     DELTA_2 += d3[i].reshape(-1,1) * A_2[i]\n",
    "    #     DELTA_1 += d2[i].reshape(-1,1) * A_1[i]\n",
    "    # DELTA_3 /= M\n",
    "    # DELTA_2 /= M\n",
    "    # DELTA_1 /= M\n",
    "    DELTA_3 = np.dot(d4.T, A_3)/M                                            # 10x33\n",
    "    DELTA_2 = np.dot(d3.T, A_2)/M                                            # 32x65\n",
    "    DELTA_1 = np.dot(d2.T, A_1)/M                                            # 64x785\n",
    "    DELTA_3[:, 1:] += lmd * THETA_3[:, 1:] / M\n",
    "    DELTA_2[:, 1:] += lmd * THETA_2[:, 1:] / M\n",
    "    DELTA_1[:, 1:] += lmd * THETA_1[:, 1:] / M\n",
    "    GRAD_ALL = reshape_theta(DELTA_1, DELTA_2, DELTA_3)\n",
    "    return GRAD_ALL\n",
    "\n",
    "def hypothesis(THETA_1, THETA_2, THETA_3, X):\n",
    "    A_4 = forward(THETA_1, THETA_2, THETA_3, X)\n",
    "    predict_results = np.zeros(A_4.shape)\n",
    "    for i in range(len(A_4)):\n",
    "        predict_results[i, np.argmax(A_4[i])] = 1\n",
    "    return predict_results\n",
    "\n",
    "def evaluate(THETA_ALL, X, Y):\n",
    "    THETA_1, THETA_2, THETA_3 = extract_theta(THETA_ALL)\n",
    "    predict_results = hypothesis(THETA_1, THETA_2, THETA_3, X)\n",
    "    diffs = np.argmax(predict_results, axis=1) - np.argmax(Y, axis=1)\n",
    "    success_count = len(diffs[diffs == 0])\n",
    "    print(\"Accuracy:\", success_count / len(X))\n",
    "\n",
    "input_count = 784\n",
    "h1_count = 64\n",
    "h2_count = 32\n",
    "output_count = 10\n",
    "\n",
    "trainData = np.loadtxt(open('digits_training.csv', 'r'), delimiter=\",\",skiprows=1)\n",
    "y_train_data = trainData[:, 0].astype(int)\n",
    "train_samples_count = len(y_train_data)\n",
    "x_train = normalize_data(trainData[:, 1:])\n",
    "x_train = np.insert(x_train, 0, np.ones(train_samples_count), axis=1)\n",
    "y_train = make_vectorized_labels(y_train_data, output_count)\n",
    "print(\"装载训练数据：\", train_samples_count, \"条\")\n",
    "print(\"训练中...\")\n",
    "\n",
    "#初始化theta\n",
    "eps = 0.12\n",
    "lmd = 0.1\n",
    "INIT_THETA_1 = np.random.random((h1_count, input_count + 1)) * 2 * eps - eps\n",
    "INIT_THETA_2 = np.random.random((h2_count, h1_count + 1))* 2 * eps - eps\n",
    "INIT_THETA_3 = np.random.random((output_count, h2_count + 1))* 2 * eps - eps\n",
    "INIT_THETA_ALL = reshape_theta(INIT_THETA_1, INIT_THETA_2, INIT_THETA_3)\n",
    "\n",
    "result = opt.minimize(cost, INIT_THETA_ALL, args=(x_train, y_train, lmd), jac=grad, method='L-BFGS-B',options={'maxiter':1000})\n",
    "print(\"训练完成：\", result)\n",
    "# import bgd_resolver as bgd\n",
    "# THETA_ALL = bgd.batch_gradient_descent(cost, grad, INIT_THETA_ALL, x_train, y_train)\n",
    "\n",
    "testData = np.loadtxt(open('digits_testing.csv', 'r'), delimiter=\",\",skiprows=1)\n",
    "y_test_data = testData[:, 0].astype(int)\n",
    "test_samples_count = len(y_test_data)\n",
    "x_test = normalize_data(testData[:, 1:])\n",
    "x_test = np.insert(x_test, 0, np.ones(test_samples_count), axis=1)\n",
    "y_test = make_vectorized_labels(y_test_data, output_count)\n",
    "print(\"装载测试数据：\", test_samples_count, \"条\")\n",
    "\n",
    "evaluate(result.x, x_test, y_test)      \n",
    "# lmd=0.0时，cost约0.00025, accuracy约0.926, 88次循环收敛\n",
    "# lmd=0.1时，cost约0.026, accuracy约0.952, 524次循环收敛\n",
    "# lmd=0.2时，cost约0.046, accuracy约0.948, 693次循环收敛\n",
    "# lmd=0.5时，cost约0.102, accuracy约0.948, 1000次循环未收敛\n",
    "# lmd=1.0时，cost约0.313, accuracy约0.938, 1000次循环未收敛"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
