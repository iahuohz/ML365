{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **演示0703：多变量线性回归**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **提出问题**\n",
    "在之前的实验中，披萨价格仅与直径有关，按照这一假设，其预测的结果并不令人满意(R方=0.662)。本章再引入一个新的影响因素：披萨辅料级别(此处已经把辅料级别调整成数值，以便能够进行数值计算)。训练数据如下：\n",
    "\n",
    "![](../images/070301.png)\n",
    "\n",
    "另外提供测试数据如下：\n",
    "\n",
    "![](../images/070302.png)\n",
    "\n",
    "如何使用线性回归训练数据，并且判断是否有助于提升预测效果呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **案例1：基于LinearRegression的实现**\n",
    ">  \n",
    "* 与单变量线性回归类似，但要注意训练数据此时是(是训练数据条数，是自变量个数)，在本例中，是5x2的矩阵：<code>xTrain = np.array([[6,2],[8,1],[10,0],[14,2],[18,0]])</code>\n",
    "* 针对测试数据的预测结果，其R方约为0.77，已经强于单变量线性回归的预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "假设函数参数： 1.1875 [1.01041667 0.39583333]\n",
      "测试数据预测结果与实际结果差异： [-0.9375      1.78125    -1.90625     0.14583333  2.3125    ]\n",
      "测试数据R方： 0.7701677731318468\n"
     ]
    }
   ],
   "source": [
    "''' 使用LinearRegression进行多元线性回归 '''\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "xTrain = np.array([[6, 2], [8, 1], [10, 0], [14, 2], [18, 0]])  # 无需手动添加Intercept Item项         \n",
    "yTrain = np.array([7, 9, 13, 17.5, 18])\n",
    "\n",
    "xTest= np.array([[8, 2], [9, 0], [11, 2], [16, 2], [12, 0]])\n",
    "yTest = np.array([11, 8.5, 15, 18, 11])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(xTrain, yTrain)\n",
    "hpyTest = model.predict(xTest)\n",
    "\n",
    "print(\"假设函数参数：\", model.intercept_, model.coef_)\n",
    "print(\"测试数据预测结果与实际结果差异：\", hpyTest - yTest)\n",
    "print(\"测试数据R方：\", model.score(xTest, yTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **案例2：基于成本函数和梯度下降的实现**\n",
    "对于一个自变量$x_1$的情形，$y$与$x$的关系用一条直线就可以拟合(假设有一定线性相关性)。对于有两个自变量$x_1, x_2$的情形，$y$与$x$的关系就需要用一个平面来拟合。如果有更多的自变量，虽然无法在三维空间中展现，但仍然可以用数学的方式来描述它们之间的关系。\n",
    "* 判别函数：$ h_\\theta(x)=\\theta_0 x_0+ \\theta_1 x_1+ \\theta_2 x_2+ \\cdots + \\theta_n x_n $\n",
    " * $x_0$称为Inercept Term，一般设置为1即可\n",
    " * $x_1,x_2,\\cdots,x_n$表示影响$y$的各个因素。假设共有$n$个影响因素(即$n$个维度)\n",
    "* 判别函数的矩阵表述形式：  \n",
    "对于每一个样本，都有：  \n",
    "$ h_\\theta(x^{(1)})=\\theta_0 x_0^{(1)}+ \\theta_1 x_1^{(1)}+ \\theta_2 x_2^{(1)}+ \\cdots + \\theta_n x_n^{(1)} $  \n",
    "$ h_\\theta(x^{(2)})=\\theta_0 x_0^{(2)}+ \\theta_1 x_1^{(2)}+ \\theta_2 x_2^{(2)}+ \\cdots + \\theta_n x_n^{(2)} $  \n",
    "$\\cdots \\cdots$  \n",
    "$ h_\\theta(x^{(m)})=\\theta_0 x_0^{(m)}+ \\theta_1 x_1^{(m)}+ \\theta_2 x_2^{(m)}+ \\cdots + \\theta_n x_n^{(m)} $  \n",
    "其中：$x_j^{(i)}$ 表示第$i$个样本数据的第$j$个自变量(第$j$个维度)。注意，各组数据的第0个自变量均为Intercept Term，直接设置为1  \n",
    "以矩阵运算的方式表示上面的各组公式，可得：    \n",
    "$ h_\\theta=X * \\theta $  \n",
    "其中：  \n",
    "$ X=\\left(\\begin{matrix}\n",
    "1 & x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\\n",
    "1 & x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)}\n",
    "\\end{matrix}\\right) $，$ \\theta=\\left(\\begin{matrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_n\n",
    "\\end{matrix}\\right) $\n",
    "* 成本函数的矩阵运算形式：  \n",
    "$ J(\\theta)= \\dfrac{1}{2m} \\sum_{i=1}^m(h_\\theta (x^{(i)}) - y^{(i)})^2 =\\dfrac{1}{2m} (X * \\theta - y)^T (X * \\theta - y) $\n",
    "* 梯度变化(偏导数):  \n",
    "$ \\begin{aligned}\n",
    "\\dfrac{\\partial J(\\theta)}{\\partial \\theta_0} & =\n",
    "\\dfrac{\\partial [\\dfrac{1}{2m} \\sum_{i=1}^m (\\theta_0 + \\theta_1 x_1^{(i)} + \\cdots + \\theta_n x_n^{(i)})^2 ]}{\\partial \\theta_0} \\\\ \\\\ & =\n",
    "\\dfrac{1}{m} \\sum_{i=1}^m (\\theta_0 + \\theta_1 x_1^{(i)} + \\cdots + \\theta_n x_n^{(i)}) * x_0^{(i)}\n",
    "\\end{aligned} $  \n",
    "$ \\begin{aligned}\n",
    "\\dfrac{\\partial J(\\theta)}{\\partial \\theta_1} & =\n",
    "\\dfrac{\\partial [\\dfrac{1}{2m} \\sum_{i=1}^m (\\theta_0 + \\theta_1 x_1^{(i)} + \\cdots + \\theta_n x_n^{(i)})^2 ]}{\\partial \\theta_1} \\\\ \\\\ & =\n",
    "\\dfrac{1}{m} \\sum_{i=1}^m (\\theta_0 + \\theta_1 x_1^{(i)} + \\cdots + \\theta_n x_n^{(i)}) * x_1^{(i)}\n",
    "\\end{aligned} $  \n",
    "$ \\vdots $  \n",
    "$ \\begin{aligned}\n",
    "\\dfrac{\\partial J(\\theta)}{\\partial \\theta_n} & =\n",
    "\\dfrac{\\partial [\\dfrac{1}{2m} \\sum_{i=1}^m (\\theta_0 + \\theta_1 x_1^{(i)} + \\cdots + \\theta_n x_n^{(i)})^2 ]}{\\partial \\theta_n} \\\\ \\\\ & =\n",
    "\\dfrac{1}{m} \\sum_{i=1}^m (\\theta_0 + \\theta_1 x_1^{(i)} + \\cdots + \\theta_n x_n^{(i)}) * x_n^{(i)}\n",
    "\\end{aligned} $  \n",
    "上式中，$ x_0^{(i)} = 1 $  \n",
    "将上述各组式子用矩阵形式表达如下：  \n",
    "$ \\dfrac{\\partial J(\\theta)}{\\partial \\theta}=X^T * (X * \\theta - y) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "循环 40961 次后收敛\n",
      "theta值 [1.01128555 0.39906853 1.17356105]\n",
      "测试数据预测值与真实值的差异： [-0.93801751  1.77513099 -1.90416086  0.15226688  2.30898763]\n",
      "测试数据R方： 0.7709259763685181\n"
     ]
    }
   ],
   "source": [
    "''' 批量梯度下降法实现多元线性回归 '''\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import bgd_resolver\n",
    "\n",
    "def costFn(theta, X, y):                           # 成本函数\n",
    "    temp = X.dot(theta) - y\n",
    "    return (temp.T.dot(temp)) / (2 * len(X))\n",
    "\n",
    "def gradientFn(theta, X, y):                       # 根据成本函数，分别对x0,x1...xn求导数(梯度)\n",
    "    return (X.T).dot(X.dot(theta) - y) / len(X)\n",
    "\n",
    "\n",
    "xTrainData = np.array([[6, 2], [8, 1], [10, 0], [14, 2], [18, 0]])\n",
    "yTrain = np.array([7, 9, 13, 17.5, 18])\n",
    "xTrain = np.c_[xTrainData, np.ones(len(xTrainData))]\n",
    "\n",
    "np.random.seed(0)\n",
    "init_theta = np.random.randn(xTrain.shape[1])\n",
    "theta = bgd_resolver.batch_gradient_descent(costFn, gradientFn, init_theta, xTrain, yTrain) \n",
    "print(\"theta值\", theta)\n",
    "\n",
    "xTestData = np.array([[8, 2], [9, 0], [11, 2], [16, 2], [12, 0]])\n",
    "yTest = np.array([11, 8.5, 15, 18, 11])\n",
    "xTest = np.c_[xTestData, np.ones(len(xTestData))]\n",
    "print(\"测试数据预测值与真实值的差异：\", xTest.dot(theta) - yTest)\n",
    "\n",
    "rsquare = bgd_resolver.batch_gradient_descent_rsquare(theta, xTest, yTest)\n",
    "print(\"测试数据R方：\", rsquare)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
